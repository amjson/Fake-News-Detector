{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project ~ Fake News  Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a fake news detector project. This notebook contains a model that will be able to detect if a news article is either real or fake based on the dataset that it has been trained and tested with.\n",
    "\n",
    "This is a simulation of how fake news are detected on the online platforms so as to prevent people not to fall into traps such as fraud and scam on online platforms mostly.\n",
    "\n",
    "The used dataset was extracted from kaggle.com which most of its dataset are from outside africa, but since fake news is widely spread all over the world mostly on online platform it will help us on creating a solution of this problem.\n",
    "\n",
    "The project will be presented as follows:\n",
    "* Data collection\n",
    "* Exploratory data analysis \n",
    "* Text Pre-processing\n",
    "* Data Modeling\n",
    "* Model Evaluation\n",
    "* Model Simulation\n",
    "\n",
    "## Data collection\n",
    "\n",
    "The data that i am going to use was downloaded from kaggle.com since i had no other resource to get data from, the dataset contains the following collumns:\n",
    "* Unnamed: 0\n",
    "* title\n",
    "* text\n",
    "* label\n",
    "\n",
    "#### importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pandas library\n",
    "import pandas as pd\n",
    "\n",
    "#for nltk  library\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "# for sklearn library\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# for flask\n",
    "from flask import Flask, render_template, request\n",
    "\n",
    "# command used for skipping browser warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv(\"news.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4  It's primary day in New York and front-runners...  REAL  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6335 entries, 0 to 6334\n",
      "Data columns (total 4 columns):\n",
      "Unnamed: 0    6335 non-null int64\n",
      "title         6335 non-null object\n",
      "text          6335 non-null object\n",
      "label         6335 non-null object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 198.0+ KB\n"
     ]
    }
   ],
   "source": [
    "news.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now let us confirm if their are any missing data in any column of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    0\n",
       "title         0\n",
       "text          0\n",
       "label         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "renaming the columns from there initials to the new column names that is quite simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.rename(columns = {\"title\":\"Headline\",\"text\":\"Text\",\"label\":\"Label\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now lets change the column of label from string to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.loc[news[\"Label\"] == 'REAL', \"Label\"] = 1\n",
    "news.loc[news[\"Label\"] == 'FAKE', \"Label\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets delete the Unnamed: 0 column coz its not for use in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.drop([\"Unnamed: 0\"],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  \\\n",
       "0                       You Can Smell Hillary’s Fear   \n",
       "1  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        Kerry to go to Paris in gesture of sympathy   \n",
       "3  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                Text  Label  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...      0  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...      0  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...      1  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...      0  \n",
       "4  It's primary day in New York and front-runners...      1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6335, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-processing\n",
    "\n",
    "The main aim at this stage is that the dataset have stopwords and punctuation in every text to clean the dataset I shall be removing stopwords becouse they are common words that always appear in every text that is in the dataset and it will not make much meaning training a model with them and also punctuation have no much meaning to the model so we have to remove them before procedding to the next stage\n",
    "\n",
    "Lets use Python built-in string library to see a list of some punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use Python built-in stopwords library to see some english stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "sw = stopwords.words('english')\n",
    "print(sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets use an example of a text and see how text preprocessing works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n",
      " \n",
      "What is Text Preprocessing? It is to do with removing punctuations, removing stop_words, returning a list!!!\n"
     ]
    }
   ],
   "source": [
    "protype = 'What is Text Preprocessing? It is to do with removing punctuations, removing stop_words, returning a list!!!'\n",
    "print(len(protype))\n",
    "print(\" \")\n",
    "print(protype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'is', 'Text', 'Preprocessing', 'It', 'is', 'to', 'do', 'with', 'removing', 'punctuations', 'removing', 'stopwords', 'returning', 'a', 'list']\n"
     ]
    }
   ],
   "source": [
    "# Check characters to see if they are in punctuation\n",
    "nopunctuation = [word for word in protype if word not in punctuation]\n",
    "\n",
    "# Join the characters again to form the string.\n",
    "nopunctuation = ''.join(nopunctuation)\n",
    "\n",
    "print(nopunctuation.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      " \n",
      "['Text', 'Preprocessing', 'removing', 'punctuations', 'removing', 'stopwords', 'returning', 'list']\n"
     ]
    }
   ],
   "source": [
    "# Now just remove any stopwords\n",
    "sw = stopwords.words('english')\n",
    "clean_text = [word for word in nopunctuation.split() if word.lower() not in sw]\n",
    "print(len(clean_text))\n",
    "print(\" \")\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(probook):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunctuation = [word for word in probook if word not in punctuation]\n",
    "\n",
    "    # Join the characters again to form the string\n",
    "    nopunctuation = ''.join(nopunctuation)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    sw = stopwords.words('english')\n",
    "    return [word for word in nopunctuation.split() if word.lower() not in sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Daniel Greenfield, a Shillman Journalism Fello...\n",
       "1    Google Pinterest Digg Linkedin Reddit Stumbleu...\n",
       "2    U.S. Secretary of State John F. Kerry said Mon...\n",
       "3    — Kaydee King (@KaydeeKing) November 9, 2016 T...\n",
       "4    It's primary day in New York and front-runners...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the original text\n",
    "text = news.Text\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Daniel, Greenfield, Shillman, Journalism, Fel...\n",
       "1    [Google, Pinterest, Digg, Linkedin, Reddit, St...\n",
       "2    [US, Secretary, State, John, F, Kerry, said, M...\n",
       "3    [—, Kaydee, King, KaydeeKing, November, 9, 201...\n",
       "4    [primary, day, New, York, frontrunners, Hillar...\n",
       "5    [I’m, immigrant, grandparents, 50, years, ago,...\n",
       "6    [Share, Baylee, Luciani, left, Screenshot, Bay...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the processed text\n",
    "news['Text'].head(7).apply(text_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modeling\n",
    "\n",
    "#### Train Test Split\n",
    "\n",
    "Now let's split the data into training set and testing set. We will train out model with the training set and then use the test set to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(news['Text'], news['Label'], test_size=0.3, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size = 4434\n",
      "Test size = 1901\n"
     ]
    }
   ],
   "source": [
    "print('Training size =', len(x_train))\n",
    "print('Test size =', len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization\n",
    "\n",
    "Since a computer doesn't know how to read text instead it reads 0s and 1s we will need some sort of numerical feature to convert our text into vector in order to enable the computer read and make predictions\n",
    "\n",
    "In this section we will convert the raw text into vectors using TfidfVectorizer\n",
    "\n",
    "specifying the analyzer to be the previous defined function\n",
    "\n",
    "Using our previous example of protype we are trying to see how the TfidfVectorizer will work in our dataset with this three sentences splited with a comma, we have used .fit_transform() to fit and transform the same time, then printing the output as an array to see how they have been vectorised, and also used .get_feature_names() to reveal the vectorized words in terms of appearence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = TfidfVectorizer(min_df=1,stop_words='english')\n",
    "traincheck = cv.fit_transform([\"What is Text Preprocessing?\", \"It is to do with removing punctuations\", \"removing stop_words, returning a list!!!\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now this is how the computer will read the above three sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.70710678, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.70710678],\n",
       "       [0.        , 0.        , 0.79596054, 0.60534851, 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.52863461, 0.        , 0.        , 0.40204024, 0.52863461,\n",
       "        0.52863461, 0.        ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traincheck.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets create a vectorizer that we will use in the whole project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectoriza = TfidfVectorizer(analyzer=text_process, stop_words=\"english\", decode_error=\"ignore\", max_df=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets fit in the vectorizer with the x_train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer=<function text_process at 0x00000199A2B81E18>,\n",
       "        binary=False, decode_error='ignore', dtype=<class 'numpy.float64'>,\n",
       "        encoding='utf-8', input='content', lowercase=True, max_df=0.7,\n",
       "        max_features=None, min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "        preprocessor=None, smooth_idf=True, stop_words='english',\n",
       "        strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectoriza.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PassiveAggressive Classifier\n",
    "\n",
    "Passive Aggressive Classifier are algorithms that remains passive for a correct classification outcome and turns aggressive in the event of a miscalculation, updating and adjusting also it does not converge, its purpose is to make updates that correct the loss causing very little change in the norm of the weight vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PassiveAggressiveClassifier(max_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None,\n",
       "              early_stopping=False, fit_intercept=True, loss='hinge',\n",
       "              max_iter=50, n_iter=None, n_iter_no_change=5, n_jobs=None,\n",
       "              random_state=None, shuffle=True, tol=None,\n",
       "              validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(vectoriza.transform(x_train), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Now lets see how good our model will do to the entire dataset. Let's begin by getting all the predictions\n",
    "\n",
    "finally predicting from the model using .predict() then check out the model accuracy finishing with a classification report that is a similar with accuracy but it is a matrix form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "confirmation = model.predict(vectoriza.transform(x_test))\n",
    "print(confirmation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy : 94.63%\n"
     ]
    }
   ],
   "source": [
    "why_pred = model.predict(vectoriza.transform(x_test))\n",
    "score    = accuracy_score(y_test,why_pred)\n",
    "print(f'Model Accuracy : {round(score*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.95       974\n",
      "           1       0.95      0.94      0.94       927\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      1901\n",
      "   macro avg       0.95      0.95      0.95      1901\n",
      "weighted avg       0.95      0.95      0.95      1901\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, why_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[926  48]\n",
      " [ 54 873]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, why_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Simulation\n",
    "#### Sample Prediciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article  = [\"Facebook has announced that due to the success of emoji's and how widely recognized they have become, Facebook\\\n",
    "            has opted to take the bold step of deleting all emoji's.\"]\n",
    "analyse1 = vectoriza.transform(article).toarray()\n",
    "model.predict(analyse1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article2 = [\"Muhammad Ali was deathly afraid of flying. Louisville just named its airport after him.\"]\n",
    "analyse2 = vectoriza.transform(article2).toarray()\n",
    "model.predict(analyse2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template(\"home.html\")\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():    \n",
    "    if request.method == 'POST':\n",
    "        article = request.form['article']\n",
    "        data    = [article]\n",
    "        sight   = vectoriza.transform(data).toarray()\n",
    "        analyse = model.predict(sight)\n",
    "\n",
    "        return render_template('result.html', prediction=analyse)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=8577)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
